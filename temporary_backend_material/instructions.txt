Hello, it's David. Here's are some files from my temporary backend. I'll explain how they work so that you it may help you with the actual backend. 

I used the server from labs as my temporary backend, with Postgres as the database. A new table, speakersV2, was created for our app. It looks like this

id -> INT
name -> VARCHAR(255)
relationship -> VARCHAR(255)
photo -> TEXT

speaker_recognition.py 

This is basically features.py, interface.py, skgmm.py, speaker-recognition.py, utils.py from speaker-recognition-py3 all combined into one. 

I changed the function task_predict so that it only takes in one input file. It also returns the label and score instead of just printing them out. Note that the softmax function which calculates the score may result in a division by when the model is lightly trained. 

views.py

There are some leftover code from the labs. Just read the ones that have V2 in it.

getSpeakersV2 ***

Front -> Back
{}

Back -> Front
{"speakers": [[id_1,name_1,relationship_1,photo_1],[id_2, ...]... ]}

It handles a GET request. Returns the entire table of speakersV2 to the frontend. That way, the frontend can display everyone's profile in the community scene. 

addVoiceV2 ***

Front -> Back
{
	"id": An integer representing the speaker's id starting from 1. 0 indicates a new speaker registration,
	"name": A string holding the name of the speaker,
	"relationship": A string holding the relationship of the speaker to the patient
	"audio": An audio string encoded in the aac format,
	"photo": A string holding the photo
 }

Back -> Front
{}

It handles a POST request. 
If the id is 0, it'll insert a new row into the speakersV2 table. All 5 parameters are required.
If the id is not 0, only id and audio are required. 
A folder is created for each speaker, holding audio clips of that speaker. I used the speaker's id as each folder's name.
These folders are used for training the model.
Note that I'm currently deleting and rebuilding the model each time a new voice clip is enrolled. 
I encountered a strange bug where the model always ouputs the most recently enrolled voice clip's speaker as its prediction if I 
enrolled the voice clips one by one. That's why I decided to rebuild the model upon each request and enroll all the voice clips at once.
Another important note: swift couldn't write audio files in wav format. Their written in Apple's own format called aac. 
My solution is to convert these aac files into wav file with the ffmpeg command. 

identifyV2 ***

Front -> Back
{
	"audio": An audio string
}

Back -> Front
{
	"name": A string holding the identified speaker's name
	"relationship": A string holding the identified speaker's relationship
	"photo": A string holding the speaker's photo 
	"label" A string holding the label. Note that it's a string not an int
}

It handles a POST request.
I kept the label as a string so that it's easier to decode in the frontend.
The label isn't necessarily needed, I only included it for debugging purposes.
If a speaker is successfully identified, his/her name, relationship, photo, and label will be sent to the frontend.
If not, the label will be "0" while the other parameters will hold an empty string "".







